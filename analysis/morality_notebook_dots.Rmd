---
title: "Morality in the time of cognitive famine - Dots task"
author: "Jonas Kristoffer LindelÃ¸v"
date: "December, 2018"
---


# About
This is part of the analysis that accompanies the paper "Morality in the time of cognitive famine" by Panos, Jonas, Michaela, and others. You are now looking at the analysis of **experiment 2** using the Dots Task to study dishonesty (cheating) 



# Setting up
Load appropriate stuff:
```{r setup, results=FALSE, message=FALSE, warning=FALSE}
library(car)  # For some of the logistic regressions
library(tidyverse)
library(lme4)
library(kableExtra)

source('misc/functions utility.R')
source('misc/functions inference.R')  # Contains LRT and LRT_binom
```

You could redo the preprocessing of the original data if you wanted to. It saves the data.frame in an .Rda file which is loaded in the sections below.
```{r preprocess dots, eval=FALSE, warning=FALSE, message=FALSE}
source('preprocess dots.R')
```

# Load data and remove some unneccessary columns
```{r load dots}
D_dots = readRDS('data/dots.Rda')
D_dots = droplevels(subset(D_dots, condition == 'experiment'))  # Not practice, etc
D_dots = select(D_dots, -encode, -equationCorrect, -equationScores, -equationRTs, -recallAns, -equationAnss)  # Tidyr doesn't like these matrix columns and we won't
D = D_dots  # For convenience
```



# Descriptives
For manuscript:
```{r dots descriptives}
# Just one row per subject to do descriptives on them
D_id = D[!duplicated(D$id), ]

# Per-group descriptives for paper
D_id %>%
  group_by(exp_part) %>%
  summarise(n=n(),
            age_years = sprintf('%.1f (%.1f)', mean(age), sd(age)),
            males = sprintf('%i (%.1f %%)', sum(gender=='male'), 100*sum(gender=='male')/n())) #%>%
  #kable() %>% 
  #kable_styling(bootstrap_options = "striped", full_width = F)
```

All groups stratified by level and stimType for supplementary:
```{r}
x = D_id %>%  # Per-person
  group_by(stimType, level) %>%
  summarise(n=n(),
            males = sprintf('%i (%.1f %%)', sum(gender=='male'), 100*sum(gender=='male')/n()),
            age_years = sprintf('%.1f (%.1f)', mean(age), sd(age)))

y = D %>%  # All trials
  group_by(stimType, level) %>%
  summarise(arithmetic = sprintf('%.1f%%', mean(equationCorrectness)*100),
            recall = sprintf('%.1f%%', mean(recallProportion)*100))

table_S2 = bind_cols(x, y[,3:4]) 
write.csv(table_S2, "figures/Table S2 - Dots group descriptives.csv", row.names = FALSE)

#%>%
  #kable() %>% 
  #kable_styling(bootstrap_options = "striped", full_width = F)
```





# WM Load and dots: Figure 4 and supplementary
```{r figure4, cache=TRUE}
# Span and correctness
D$levelGroup = ifelse(D$level == '1-7', 'span 1-7', 'span 1-3, 3-5, 5-7')
D$levelGroup = factor(D$levelGroup, levels=c('span 1-7', 'span 1-3, 3-5, 5-7'))
D$incentiveLevel = with(D, interaction(dotsIncentive, level))

# The figure
# Note that binom.summary is hand-made (in utilities) to do binomial CIs.
figure4 = ggplot(D, aes(x=span, y=dotsCorrect, color=dotsIncentive, group=incentiveLevel)) +
  stat_summary(fun.data=binom.summary, size=0.3, position=position_dodge(0.5)) + 
  stat_summary(fun.y=mean, geom='line', position=position_dodge(0.5)) +
  
  # Alternative way of showing the trends. 
  # Closer to the inference model but further from the data.
  #binomial_smooth() + 
  #stat_summary(fun.y = mean, geom='point') + 
  
  # Styling
  geom_hline(aes(yintercept=0.5), color='red', lty=2) +
  scale_x_continuous(breaks=1:7) + scale_y_continuous(breaks=seq(-1, 1, 0.1 )) + 
  coord_cartesian(ylim=c(0.4, 1)) + 
  
  # Faceting
  facet_grid(levelGroup~dotsReceiver) +
  
  # Labels
  labs(title='WM load and cheating', y='Correctness', x='Complex Span')

# More styling
figure4 = style_my_plot(figure4) + 
   scale_colour_manual(values=c('black', '#777777'))
  
# Save and plot it
ggsave('figures/Figure 4 - Dots and CS span.png', figure4, width=7, height=7, units='cm', dpi=300, scale=1.7)
figure4
```


Supplementary figure - adds `dotsN` and `stimType` to the faceting:
```{r figure4 supplementary}
figure4S = ggplot(D, aes(x=span, y=dotsCorrect, color=dotsIncentive)) + 
  stat_summary(fun.data=binom.summary, size=0.3, position=position_dodge(0.5)) + 
  stat_summary(fun.y=mean, geom='line', position=position_dodge(0.5)) +
  
  # Styling
  geom_hline(aes(yintercept=0.5), color='red', lty=2) +
  scale_x_continuous(breaks=1:7) + scale_y_continuous(breaks=seq(-1, 1, 0.1 )) + 
  coord_cartesian(ylim=c(0.25, 1)) + 
  labs(title='WM load and cheating', y='Correctness', x='Complex Span') +
  
  # Layout
  facet_grid(dotsN ~ stimType + dotsReceiver)

# More styling
figure4S = style_my_plot(figure4S) + 
  scale_colour_manual(values=c('black', '#777777'))

# Save it and show it here
ggsave('figures/Figure 4S - Dots and CS span.png', figure4S, width=9, height=9, units='cm', dpi=300, scale=1.7)
figure4S
```



# WM load and dots: inferential stuff
*Note on identifiability:* There is a tradeoff to be made in the models below between having a full random structure and making the models identifiable. I have tried keeping the model fairly maximal and verified that simpler models yield the same qualitative conclusions as the unidentifiable complex ones. One simplification is to remove `span` from some of the models (including doing `(1|id)` instead of `(span|id)`) because, as it turns out, `span` does not make a difference for the chance of getting a correct `dotsCorrect`

Do subjects cheat? dotsIncentive makes this a difference-question between cheating and accuracy incentives (testing `dotsIncentive`).

Notice that the reference level becomes quite a specific combination of the fixed effects, but we are only interested in the effects, not the intercept, so that's OK.

```{r binom incentive, warning=FALSE, cache=TRUE}
LRT_binom(D,
          dotsCorrect ~ dotsIncentive + dotsReceiver + stimType + dotsN + time_hours + (1|id),
          dotsCorrect ~                 dotsReceiver + stimType + dotsN + time_hours + (1|id))
```

Is cheating (the accuracy-cheating difference) modulated by receiver? i.e., do participants cheat equally for personal gain and for the gain of another?
```{r binom incentive-receiver, warning=FALSE, cache=TRUE}
LRT_binom(D,
          dotsCorrect ~ dotsIncentive * dotsReceiver + dotsN + time_hours + (1|id),
          dotsCorrect ~ dotsIncentive + dotsReceiver + dotsN + time_hours + (1|id))
```


Is cheating (the accuracy-cheating difference) modulated by CS span? I.e., do participants cheat equally across WM loads?
```{r binom incentive-span, warning=FALSE, cache=TRUE}
LRT_binom(subset(D, dotsReceiver == 'Self'),
          dotsCorrect ~ dotsIncentive*span + dotsN + time_hours + (1|id),
          dotsCorrect ~ dotsIncentive+span + dotsN + time_hours + (1|id))
LRT_binom(subset(D, dotsReceiver == 'Other'),
          dotsCorrect ~ dotsIncentive*span + dotsN + time_hours + (1|id),
          dotsCorrect ~ dotsIncentive+span + dotsN + time_hours + (1|id))
```


Does this cheating-span relationship replicate across dotsN (dots ambiguity)?
```{r binom incentive-span-dotsN, warning=FALSE, cache=TRUE}
LRT_binom(D,
          dotsCorrect ~ dotsIncentive*span*dotsN + dotsReceiver + time_hours + (1|id),
          dotsCorrect ~ dotsIncentive*span*dotsN + dotsReceiver + time_hours + (1|id) - dotsIncentive:span:dotsN)

```


Is this relationship different for different stimulus types?
```{r binom incentive-span-type, warning=FALSE, cache=TRUE}
LRT_binom(D,
          dotsCorrect ~ dotsIncentive*span*stimType + dotsN + (1|id),
          dotsCorrect ~ dotsIncentive*span*stimType + dotsN + (1|id) - dotsIncentive:span:stimType)
```



# Prepare for analyses of time-load
Before we dive into time-effects, we'll make a data.frame called `D_time` which excludes subjects who completed before the time window of interest - and optionally those who were very slow. For the latter, you could think that they were relaxed (not depleted) OR slowed by depletion itself, so we think that it is ambiguous.

```{r dots depletion prep}
time_frame_lower = 45 * 60  # Which time frame to analyze. Try setting to 60 minutes.
time_frame_upper = 100 * 60  # Not used in paper, but feel free to use it here.

# Select subjects for analysis: only those who did not complete prematurely
completion_data = aggregate(time_secs~id, D, FUN=max)  # Vector of completion times
ids_fast = completion_data$id[completion_data$time_secs < time_frame_lower]
ids_slow = completion_data$id[completion_data$time_secs > time_frame_upper]
D_time = subset(D, !id %in% ids_fast & !id %in% ids_slow)

# Tell what's happening and how many were excluded
sprintf('M=%.02f minutes (SD=%.02f). %i excluded out of %i', 
        mean(completion_data$time_secs)/60, 
        sd(completion_data$time_secs)/60,
        length(ids_slow) + length(ids_fast), nrow(completion_data))

# Visualize what happened :-)
plot(sort(completion_data$time_secs)/60, ylim=c(20, 110), xlab='Participant (ordered)', ylab='Completion time (minutes)', main='Window of acceptance for completion times'); grid(); abline(h=c(time_frame_lower/60, time_frame_upper/60), col='red')

# One subject has wrong/absent recording of trial time, so is not included
D_time = subset(D_time, !is.na(D_time$utc_start))
```



# Time-effects on dots: figure 5 and supplementary
For main text:
```{r figure5, cache=TRUE, fig.height=8, fig.width=5, warning=FALSE, message=FALSE}
figure5 = ggplot(D_time, aes(x=time_secs/60, y=dotsCorrect, color=dotsIncentive, shape=dotsIncentive)) + 
  stat_summary_bin(fun.y=mean, geom='point', binwidth=4, size=1.5) + 
  binomial_smooth(size=0.5) +
  #stat_smooth(method='loess', geom='line', span=1) + 
  #stat_summary_bin(fun.data=binom.summary, binwidth=10, position=position_dodge(3), size=0.3) +
  #binomial_smooth(formula = y ~ splines::ns(x, 2)) +
  
  # Layout
  facet_grid(level~dotsReceiver) + 
  
  # Styling
  geom_hline(aes(yintercept=0.5), color='red', lty=2) +
  coord_cartesian(xlim=c(0, 46), ylim=c(0.35, 1)) +
  scale_x_continuous(breaks=seq(0, 100, 10)) + 
  scale_y_continuous(breaks=seq(0,1,0.1), labels=paste(seq(0, 1, 0.1)*100, '%', sep='')) +
  
  labs(title='Load over time and cheating', y='Correctness', x='Minutes elapsed')

# Style it a bit more
figure5 = style_my_plot(figure5) + 
  scale_colour_manual(values=c('black', '#777777'))

# Save it and show it here
ggsave('figures/Figure 5 - Dots and load over time.png', figure5, width=5, height=7.5, units='cm', dpi=300, scale=2.5)
figure5
```


For the supplementary materials:
```{r figure5S, cahce=TRUE, fig.height=6, fig.width=8}
# SUPPLEMENTARY
figureS5 = ggplot(D, aes(x=time_secs/60, y=dotsCorrect, color=dotsIncentive)) + 
  #stat_summary_bin(geom='line', fun.y=mean, binwidth=1) +
  #stat_smooth(method='loess', geom='line', span=1) + 
  #stat_summary_bin(fun.data=binom.summary, binwidth=10, position=position_dodge(3), size=0.3) +
  stat_summary_bin(fun.y=mean, geom='point', binwidth=10, size=1.5) + 
  binomial_smooth(size=0.5) +
  
  # Layout
  facet_grid(level ~ dotsReceiver + dotsN) + 
  
  # Styling
  geom_hline(aes(yintercept=0.5), color='red', lty=2) +
  coord_cartesian(xlim=c(0, 60), ylim=c(0.35, 1)) +
  scale_x_continuous(breaks=seq(0, 100, 15)) + 
  scale_y_continuous(breaks=seq(0,1,0.1), labels=paste(seq(0, 1, 0.1)*100, '%', sep='')) + 
  
  labs(title='Load over time and cheating', y='Correctness', x='Minutes elapsed')

figureS5 = style_my_plot(figureS5) + 
  scale_colour_manual(values=c('black', '#777777'))
ggsave('figures/Figure S5 - Dots and load over time.png', figureS5, width=9, height=7, units='cm', dpi=300, scale=2)

figureS5
```



# Effects of time (depletion, learning, etc.)
`dotsIncentive` marks the difference between accuracy and cheat-inventive trials; i.e. the magnitude of cheating. time_hours is the time passed since first trial, scaled to facilitate identifiability in the model. The random effects structure captures some known design features on which shrinkage would be nice.
* `dotsN` is fixed because we do not consider them to be drawn randomly from a larger possible number of dots and therefore do not want shrinkage of extreme N.
* `stimType` is random because we think there could be many different kinds of stimuli which would show the same pattern.
* slopes of `time_hours` are random for each subject and there is no slope on `span` (makes the model unidentifiable). We think that there should be shrinkage to an overall time-trend.
* There is also a random offset for each subject; reflecting good/bad days, I guess.


Is there a time-effect of the cheat-incentive overall?
```{r binom time-incentive, warning=FALSE, cache=TRUE}
LRT_binom(D_time,
          dotsCorrect ~ time_hours*dotsIncentive + dotsN + dotsReceiver + (time_hours|id),
          dotsCorrect ~ time_hours+dotsIncentive + dotsN + dotsReceiver + (time_hours|id))
```



Is there a time-effect on the ability to do the task (depletion or learning)?
```{r binom time-incentive accuracy, warning=FALSE, cache=TRUE}
LRT_binom(subset(D_time, dotsIncentive == 'accuracy'),
          dotsCorrect ~ time_hours + dotsN + dotsReceiver + (time_hours|id),
          dotsCorrect ~          1 + dotsN + dotsReceiver + (time_hours|id))
```

Is there a time-effect on the "ability to cheat", i.e., does cheating increase over time?
```{r binom time-incentive cheat, warning=FALSE, cache=TRUE}
LRT_binom(subset(D_time, dotsIncentive == 'cheat'),
          dotsCorrect ~ time_hours + dotsN + dotsReceiver + (time_hours|id),
          dotsCorrect ~          1 + dotsN + dotsReceiver + (time_hours|id))
```



Do the different difficulty levels cause different time-effects?
```{r binom time-incentive-level, warning=FALSE, cache=TRUE}
D_time$level_numeric = as.numeric(D_time$level)  # Numeric to make it ordered according to the hypothesis
LRT_binom(subset(D_time, level_numeric != 4),  # Leave out the mixed group (1-7)
          dotsCorrect ~ time_hours*dotsIncentive*level_numeric + dotsN + dotsReceiver + (time_hours|id),
          dotsCorrect ~ time_hours*dotsIncentive*level_numeric + dotsN + dotsReceiver + (time_hours|id) - time_hours:dotsIncentive:level_numeric)
```


Does perceived depletion alter the observed slope change?
```{r binom time-incentive-subjective, warning=FALSE, cache=TRUE}
LRT_binom(subset(D_time, !is.na(subjectiveDepletion)),
          dotsCorrect ~ time_hours*dotsIncentive*subjectiveDepletion + dotsN + dotsReceiver + (time_hours|id),
          dotsCorrect ~ time_hours*dotsIncentive*subjectiveDepletion + dotsN + dotsReceiver + (time_hours|id) - time_hours:dotsIncentive:subjectiveDepletion)
```


Is the time-trend on cheating different for self- and other-receivers?
```{r binom time-incentive-receiver, warning=FALSE, cache=TRUE}
LRT_binom(D_time,
          dotsCorrect ~ time_hours*dotsIncentive*dotsReceiver + dotsN + (time_hours|id),
          dotsCorrect ~ time_hours*dotsIncentive*dotsReceiver + dotsN + (time_hours|id) - time_hours:dotsIncentive:dotsReceiver)
```



